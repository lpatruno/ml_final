{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to code a routine to perform feature removal on a dataset containing categorical data. This routine is not written in scikit-learn and involved encoding the features using a One Hot Encoding scheme if categorical features are contained within the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "from time import time\n",
    "from copy import deepcopy\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer as DV\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from get_data import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = data()\n",
    "\n",
    "X = df[ [col for col in df if col not in ['label', 'class']]]\n",
    "y = df['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def n_elt_subset(S,n):\n",
    "    \"\"\"\n",
    "    Return all of the n element subsets of S\n",
    "    \"\"\"\n",
    "    return set(itertools.combinations(S, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def feature_removal(model, X, y):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : pandas DataFrame\n",
    "        DataFrame containing the features\n",
    "    y : pandas Series\n",
    "        Series containing the class labels\n",
    "    \"\"\"\n",
    "    total_time = 0\n",
    "    \n",
    "    features = X.columns\n",
    "    #print 'Features: {}'.format(features)\n",
    "    \n",
    "    # Binarize the categorical data using a DictVectorizer\n",
    "    # This requires the data be fed in the form of Python dicts\n",
    "    vectorizer = DV(sparse=False)\n",
    "    X_one_hot = vectorizer.fit_transform( X.to_dict(orient='records') )\n",
    "    enc_feats = vectorizer.get_feature_names()\n",
    "    \n",
    "    # Split into test and train sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_one_hot, y, random_state=0)\n",
    "    \n",
    "    # Calculate first score (using error as criteria)\n",
    "    classifier = deepcopy(model)\n",
    "    t0 = time()\n",
    "    classifier.fit(X_train, y_train)\n",
    "    error = (1 - classifier.score(X_test, y_test))\n",
    "    t1 = time()\n",
    "    \n",
    "    total_time = total_time + (t1-t0)\n",
    "    \n",
    "    n_element_test_error = [(features, error)]\n",
    "    \n",
    "    for n in range(X.shape[1]-1, 0, -1):\n",
    "        # Time for this number of features\n",
    "        n_feature_time = 0\n",
    "        \n",
    "        # Get list of all n-element subsets of column labels\n",
    "        n_feature_subsets = n_elt_subset(features, n)\n",
    "        errors = []\n",
    "        \n",
    "        # Loop through each list of n-element column labels\n",
    "        for feature_list in n_feature_subsets:\n",
    "            \n",
    "            # Get list of indices of the features\n",
    "            # This is non-trivial because of the way the OneHotEncoder \n",
    "            # encodes the categorical data\n",
    "            feature_indices = []\n",
    "            \n",
    "            for feature in feature_list:\n",
    "                if X[feature].dtype == object:\n",
    "                    enc_format = feature + '='\n",
    "                    cat_indices = [enc_feats.index(col) for col in enc_feats if enc_format in col]\n",
    "                    feature_indices += cat_indices\n",
    "                else:\n",
    "                    feature_indices.append(enc_feats.index(feature))\n",
    "                    \n",
    "            # Get subset of X_train and X_test corresponding to the right features\n",
    "            X_train_sub = X_train[:, feature_indices]\n",
    "            X_test_sub = X_test[:, feature_indices]\n",
    "            \n",
    "            # Fit and get testing error\n",
    "            # Calculate first score (using error as criteria)\n",
    "            classifier = deepcopy(model)\n",
    "            t0 = time()\n",
    "            classifier.fit(X_train_sub, y_train)\n",
    "            error = (1 - classifier.score(X_test_sub, y_test))\n",
    "            t1 = time()\n",
    "            \n",
    "            n_feature_time = n_feature_time + (t1-t0)\n",
    "            total_time = total_time + (t1-t0)\n",
    "            \n",
    "            errors.append((feature_list, error))\n",
    "            \n",
    "        # Sort the list of tuples\n",
    "        errors.sort(key=lambda x:x[1])\n",
    "        # Get the best performing subset of features and the associated test error\n",
    "        best_subset = errors[0]\n",
    "        # Get the best model with one more feature\n",
    "        previous_best = n_element_test_error[-1]\n",
    "        \n",
    "        print 'Number features: {} \\tError: {}'.format(n, best_subset[1])\n",
    "        print 'Time {}'.format(n_feature_time)\n",
    "        \n",
    "        #if best_subset[1] <= previous_best[1]:\n",
    "        #    features = best_subset[0]\n",
    "        #    n_element_test_error.append(best_subset)\n",
    "        #else:\n",
    "        #    print 'Test error increased with {} features'.format(n, best_subset)\n",
    "        #    break\n",
    "        \n",
    "        # Don't break out of this, get the best model for each num_feats\n",
    "        features = best_subset[0]\n",
    "        n_element_test_error.append(best_subset)\n",
    "        \n",
    "    print 'Total time taken: {}\\n'.format(total_time)\n",
    "    \n",
    "    n_element_test_error.sort(key=lambda x:x[1])\n",
    "    \n",
    "    return len(n_element_test_error[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression L1\n",
      "Number features: 13 \tError: 0.148261884289\n",
      "Time 12.4773414135\n",
      "Number features: 12 \tError: 0.147893379192\n",
      "Time 13.8209190369\n",
      "Number features: 11 \tError: 0.147647709127\n",
      "Time 5.10096788406\n",
      "Number features: 10 \tError: 0.148507554355\n",
      "Time 5.71622419357\n",
      "Number features: 9 \tError: 0.148876059452\n",
      "Time 6.00334906578\n",
      "Number features: 8 \tError: 0.149613069647\n",
      "Time 7.68797016144\n",
      "Number features: 7 \tError: 0.149981574745\n",
      "Time 6.82283711433\n",
      "Number features: 6 \tError: 0.151455595136\n",
      "Time 8.57038283348\n",
      "Number features: 5 \tError: 0.152561110429\n",
      "Time 3.97136473656\n",
      "Number features: 4 \tError: 0.156123326373\n",
      "Time 3.55817890167\n",
      "Number features: 3 \tError: 0.16275641813\n",
      "Time 3.47835922241\n",
      "Number features: 2 \tError: 0.181918683208\n",
      "Time 2.67846369743\n",
      "Number features: 1 \tError: 0.219629038202\n",
      "Time 1.60918569565\n",
      "Total time taken: 81.8102560043\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print 'Logistic Regression L1'\n",
    "feature_removal(LogisticRegression(penalty='l1'), X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression L2\n",
      "\n",
      "Number features: 13 \tError: 0.149121729517\n",
      "Time 5.74490332603\n",
      "Number features: 12 \tError: 0.146542193834\n",
      "Time 8.87190127373\n",
      "Number features: 11 \tError: 0.149121729517\n",
      "Time 7.48032784462\n",
      "Number features: 10 \tError: 0.149121729517\n",
      "Time 6.07888197899\n",
      "Number features: 9 \tError: 0.14973590468\n",
      "Time 4.89407992363\n",
      "Number features: 8 \tError: 0.150104409778\n",
      "Time 3.58520698547\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-832eb0b361b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m'Logistic Regression L2'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mfeature_removal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'l2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-e467d67d5ce2>\u001b[0m in \u001b[0;36mfeature_removal\u001b[0;34m(model, X, y)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0mclassifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0mt0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_sub\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m             \u001b[0merror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_sub\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mt1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/luigi/Documents/DataScience/venv/lib/python2.7/site-packages/sklearn/linear_model/logistic.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1152\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_intercept\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintercept_scaling\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdual\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1154\u001b[0;31m                 self.max_iter, self.tol, self.random_state)\n\u001b[0m\u001b[1;32m   1155\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn_iter_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1156\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/luigi/Documents/DataScience/venv/lib/python2.7/site-packages/sklearn/svm/base.pyc\u001b[0m in \u001b[0;36m_fit_liblinear\u001b[0;34m(X, y, C, fit_intercept, intercept_scaling, class_weight, penalty, dual, verbose, max_iter, tol, random_state, multi_class, loss, epsilon)\u001b[0m\n\u001b[1;32m    914\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_ind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misspmatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    915\u001b[0m         \u001b[0mclass_weight_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'i'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 916\u001b[0;31m         epsilon)\n\u001b[0m\u001b[1;32m    917\u001b[0m     \u001b[0;31m# Regarding rnd.randint(..) in the above signature:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    918\u001b[0m     \u001b[0;31m# seed for srand in range [0..INT_MAX); due to limitations in Numpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print 'Logistic Regression L2'\n",
    "print\n",
    "feature_removal(LogisticRegression(penalty='l2'), X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get best 12 features using Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n = 12\n",
    "features = X.columns    \n",
    "\n",
    "# Binarize the categorical data using a DictVectorizer\n",
    "# This requires the data be fed in the form of Python dicts\n",
    "vectorizer = DV(sparse=False)\n",
    "X_one_hot = vectorizer.fit_transform( X.to_dict(orient='records') )\n",
    "enc_feats = vectorizer.get_feature_names()\n",
    "\n",
    "# Split into test and train sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_one_hot, y)\n",
    "\n",
    "\n",
    "# Get list of all n-element subsets of column labels\n",
    "n_feature_subsets = n_elt_subset(features, n)\n",
    "errors = []\n",
    "\n",
    "# Loop through each list of n-element column labels\n",
    "for feature_list in n_feature_subsets:\n",
    "\n",
    "    # Get list of indices of the features\n",
    "    # This is non-trivial because of the way the OneHotEncoder \n",
    "    # encodes the categorical data\n",
    "    feature_indices = []\n",
    "\n",
    "    for feature in feature_list:\n",
    "        if X[feature].dtype == object:\n",
    "            enc_format = feature + '='\n",
    "            cat_indices = [enc_feats.index(col) for col in enc_feats if enc_format in col]\n",
    "            feature_indices += cat_indices\n",
    "        else:\n",
    "            feature_indices.append(enc_feats.index(feature))\n",
    "\n",
    "    # Get subset of X_train and X_test corresponding to the right features\n",
    "    X_train_sub = X_train[:, feature_indices]\n",
    "    X_test_sub = X_test[:, feature_indices]\n",
    "\n",
    "    # Fit and get testing error\n",
    "    # Calculate first score (using error as criteria)\n",
    "    classifier = LogisticRegression()\n",
    "    t0 = time()\n",
    "    classifier.fit(X_train_sub, y_train)\n",
    "    error = (1 - classifier.score(X_test_sub, y_test))\n",
    "    t1 = time()\n",
    "\n",
    "    errors.append((feature_list, error))\n",
    "\n",
    "# Sort the list of tuples\n",
    "errors.sort(key=lambda x:x[1])\n",
    "# Get the best performing subset of features and the associated test error\n",
    "best_subset = errors[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('age',\n",
       "  'workclass',\n",
       "  'education',\n",
       "  'education-num',\n",
       "  'marital-status',\n",
       "  'occupation',\n",
       "  'race',\n",
       "  'sex',\n",
       "  'capital-gain',\n",
       "  'capital-loss',\n",
       "  'hours-per-week',\n",
       "  'native-country'),\n",
       " 0.14678786389878395)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errors[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
