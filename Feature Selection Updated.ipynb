{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import itertools\n",
    "from time import time\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.feature_extraction import DictVectorizer as DV\n",
    "from sklearn.grid_search import ParameterGrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from get_data import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class FeatureSelection:\n",
    "    \n",
    "    def __init__(self, df, classifiers):\n",
    "        self.total_time = 0\n",
    "        self.df = df\n",
    "        self.classifiers = classifiers\n",
    "        # Split and save train, validation and test splits\n",
    "        self._split_data(train_size=.6, test_size=.4)\n",
    "        \n",
    "    def select_features(self):\n",
    "        \"\"\"\n",
    "        Perform a feature selection over a list of classifiers\n",
    "        with certain hyperparameters set.\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        # Loop over all the classifiers and hyperparameters\n",
    "        for model in classifiers:\n",
    "                \n",
    "            classifier_name = model['classifier_name']\n",
    "            param_grid = list(ParameterGrid(model['param_grid']))\n",
    "            \n",
    "\n",
    "            for params in param_grid:\n",
    "                # Initialize the model\n",
    "                classifier = copy.deepcopy( model['classifier'] )\n",
    "                # Initialize a model with the hyperparameters\n",
    "                classifier = classifier(**params)\n",
    "                print('\\n\\n{}'.format(params))\n",
    "                # Get subset of features that minimizes the validation error \n",
    "                features, error = self._feature_reduce(classifier)\n",
    "                # Save results\n",
    "                classifier_result = {'classifier': classifier_name, 'params':str(params), 'features': '{}'.format(features), 'error':error}\n",
    "                results.append(classifier_result)\n",
    "                \n",
    "        return results\n",
    "    \n",
    "        \n",
    "    def _split_data(self, train_size=.8, test_size=.2):\n",
    "        \"\"\"\n",
    "        Encode the categorical variables using a One-Hot encoding\n",
    "        and split the data into training, validation and test splits.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        train_size : float\n",
    "            Pct of data to use as training samples\n",
    "        test_size : float\n",
    "            Pct of data to use as validation and test samples\n",
    "        \"\"\"\n",
    "        # Get features and target\n",
    "        X = self.df[ [col for col in df if col not in ['label', 'class']]]\n",
    "        y = df['class'].values\n",
    "        \n",
    "        # Binarize the categorical data using a DictVectorizer\n",
    "        # This requires the data be fed in the form of Python dicts\n",
    "        vectorizer = DV(sparse=False)\n",
    "        print('Encoding features...')\n",
    "        X_binarized = vectorizer.fit_transform(X.to_dict(orient='records'))\n",
    "        \n",
    "        # Split into train, cv and test sets\n",
    "        X_train, X_cv_test, y_train, y_cv_cv = train_test_split(X_binarized, y, \n",
    "                                                            train_size=train_size, test_size=test_size)\n",
    "        X_cv, X_test, y_cv, y_test = train_test_split(X_cv_test, y_cv_cv, \n",
    "                                                            train_size=(test_size/2), test_size=(test_size/2))\n",
    "        \n",
    "        self.encoded_features = vectorizer.get_feature_names()\n",
    "        self.feature_labels = X.columns\n",
    "        self.feature_types = X.dtypes.to_dict()\n",
    "        self.X = X\n",
    "        self.X_train = X_train\n",
    "        self.X_cv = X_cv\n",
    "        self.X_test = X_test\n",
    "        self.y_train = y_train\n",
    "        self.y_cv = y_cv\n",
    "        self.y_test = y_test\n",
    "        \n",
    "        print('Number of training samples: {}'.format(X_train.shape[0]))\n",
    "        print('Number of validation samples: {}'.format(X_cv.shape[0]))\n",
    "        print('Number of test samples: {}'.format(X_test.shape[0]))\n",
    "    \n",
    "        \n",
    "    def _feature_reduce(self, classifier):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        # Get the best number of features\n",
    "        n = self._get_best_n(classifier)\n",
    "        \n",
    "        # Get all subsets of features of size n\n",
    "        n_element_features = self._n_element_subsets(self.feature_labels, n)\n",
    "        \n",
    "        # Get best n features\n",
    "        best_features, error = self._best_n_features(classifier, n_element_features, test_set=True)\n",
    "        \n",
    "        print 'Total time taken: {}'.format(self.total_time)\n",
    "        print 'Lowest Test error: {}'.format(error)\n",
    "        \n",
    "        return best_features, error\n",
    "        \n",
    "        \n",
    "    def _n_element_subsets(self, S, n):\n",
    "        \"\"\"\n",
    "        Return all of the n element subsets of S\n",
    "        \"\"\"\n",
    "        return set(itertools.combinations(S, n))\n",
    "    \n",
    "        \n",
    "    def _get_best_n(self, classifier):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        # Data for training and validating the model\n",
    "        X_train = self.X_train\n",
    "        y_train = self.y_train\n",
    "        X_cv = self.X_cv\n",
    "        y_cv = self.y_cv\n",
    "        \n",
    "        features = self.feature_labels\n",
    "        \n",
    "        # Save the features and the error for the best classifier\n",
    "        # with n elements for each n\n",
    "        best_n_model = []\n",
    "        \n",
    "        # Calculate error using all of the features\n",
    "        classifier = copy.deepcopy(classifier)\n",
    "        t0 = time()\n",
    "        classifier.fit(X_train, y_train)\n",
    "        error = (1 - classifier.score(X_cv, y_cv))\n",
    "        t1 = time()\n",
    "        self.total_time += (t1-t0)\n",
    "        \n",
    "        print 'Training error for {} features: {}'.format(self.X.shape[1], error)\n",
    "        \n",
    "        best_n_model.append( (features, error) )\n",
    "        \n",
    "        # Get the best n features for each n\n",
    "        for n in range(self.X.shape[1]-1, 0, -1):\n",
    "            # Get list of all n-element subsets of column labels\n",
    "            n_feature_subsets = self._n_element_subsets(features, n)\n",
    "            \n",
    "            # Get best set of features of size n and lowest error\n",
    "            best_n_features, best_n_error = self._best_n_features(classifier, n_feature_subsets)\n",
    "            print 'Training error: {}'.format(best_n_error)\n",
    "            best_n_model.append((best_n_features, best_n_error))\n",
    "            \n",
    "            # Reset feature to the best features from this model\n",
    "            features = best_n_features\n",
    "        \n",
    "        best_n_model.sort(key=lambda x:x[1])\n",
    "        print 'Total time taken: {}\\n'.format(self.total_time)\n",
    "    \n",
    "        print 'Best n: {}'.format(len(best_n_model[0][0]))\n",
    "        return len(best_n_model[0][0])\n",
    "    \n",
    "    \n",
    "    def _best_n_features(self, classifier, feature_subsets, test_set=False):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        print 'Calculating best {} features'.format(len(list(feature_subsets)[0]))\n",
    "        \n",
    "        enc_feats = self.encoded_features\n",
    "        \n",
    "        # Keep track of the error for each of the n feature classifiers\n",
    "        n_feature_errors = []\n",
    "        \n",
    "        # Loop through each list of n-element column features\n",
    "        for feature_list in feature_subsets:\n",
    "            \n",
    "            # Get list of indices of the features\n",
    "            # Non-trivial due to the OneHotEncoder-ing of the data\n",
    "            feature_indices = []\n",
    "            \n",
    "            # Loop over each feature within the list to get types\n",
    "            for feature in feature_list:\n",
    "                if self.feature_types[feature] == object:\n",
    "                    encoded_feature_label = feature + '='\n",
    "                    encoded_feat_indices = [enc_feats.index(col) for col in enc_feats if encoded_feature_label in col]\n",
    "                    feature_indices += encoded_feat_indices\n",
    "                else:\n",
    "                    feature_indices.append(enc_feats.index(feature))\n",
    "            \n",
    "            # Fit and get error for classifier\n",
    "            classifier = copy.deepcopy(classifier)\n",
    "            \n",
    "            if test_set:\n",
    "                # Use training and cv sets to train the model\n",
    "                X_train = np.concatenate((self.X_train,self.X_cv))\n",
    "                y_train = np.concatenate((self.y_train, self.y_cv))\n",
    "                # Get subset of X_train and X_test corresponding to the right features\n",
    "                X_train = X_train[:, feature_indices]\n",
    "                \n",
    "                t0 = time()\n",
    "                classifier.fit(X_train, y_train)\n",
    "                \n",
    "                X_test = self.X_test[:, feature_indices]\n",
    "                error = (1 - classifier.score(X_test, self.y_test))\n",
    "            else:\n",
    "                # Get subset of X_train and X_test corresponding to the right features\n",
    "                X_train = self.X_train[:, feature_indices]\n",
    "                y_train = self.y_train\n",
    "        \n",
    "                t0 = time()\n",
    "                classifier.fit(X_train, y_train)\n",
    "                X_cv = self.X_cv[:, feature_indices]\n",
    "                error = (1 - classifier.score(X_cv, self.y_cv))\n",
    "                \n",
    "            t1 = time()\n",
    "            self.total_time += (t1-t0)\n",
    "            \n",
    "            n_feature_errors.append((feature_list, error))\n",
    "            \n",
    "        # Sort the feature lists and return the most performant feature list\n",
    "        n_feature_errors.sort(key=lambda x:x[1])\n",
    "        \n",
    "        return n_feature_errors[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Do a grid search for svc but not feature selection\n",
    "{'classifier': SVC, \n",
    "                'classifier_name': 'Linear SVC',\n",
    "                'param_grid': {'C': np.linspace(.1, 5, 50), 'kernel': ['linear']}\n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = data()\n",
    "\n",
    "classifiers = [{'classifier': LogisticRegression, \n",
    "                'classifier_name': 'Logistic Regression L2',\n",
    "                'param_grid': {'C': np.linspace(.1, 5, 50), 'max_iter': [10,50,100,200], 'penalty': ['l2']}\n",
    "              }]\n",
    "\n",
    "feature_selection = FeatureSelection(df, classifiers)\n",
    "\n",
    "results = feature_selection.select_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
