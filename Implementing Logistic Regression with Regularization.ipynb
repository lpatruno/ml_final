{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this notebook is to extend a previous implementation of the Logistic Regression classifier by implementing both $L1$ and $L2$ regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "from scipy.special import expit as sigmoid\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The update rule for the Maximum Likelihood Estimation is given by \n",
    "$$w_j \\leftarrow w_j + \\epsilon x_j^i(y^i - g(\\mathbf{w}^T\\mathbf{x}^i))$$\n",
    "Here, we loop over each data point $(y^i, \\mathbf{x}^i)$ and update each weight $w_j$.\n",
    "<br><br>We can choose to regularize our model to reduce overfitting. \n",
    "For example, $L1$ regularization seeks to minimize all learned weights. The update rule for $L1$ regularization is\n",
    "$$w_j \\leftarrow w_j + \\epsilon \\bigg[x_j^i(y^i - g(\\mathbf{w}^T\\mathbf{x}^i)) \n",
    "- \\frac{\\text{sign}(w_j)}{\\lambda N} \\bigg]$$\n",
    "$L2$ regularization, on the other hand, seeks to minimize the number of non-zero weights. It's update rule is given by\n",
    "$$w_j \\leftarrow w_j + \\epsilon \\bigg[x_j^i(y^i - g(\\mathbf{w}^T\\mathbf{x}^i)) \n",
    "- \\frac{w_j}{\\lambda N} \\bigg]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize a Logistic Regression Classifier object\n",
    "        \n",
    "        Params\n",
    "        ---------\n",
    "        weights - Learned weights for the Logistic Regression.\n",
    "            Fit to the training data by calling the fit method.\n",
    "            \n",
    "        _epsilon - Hyperparameter for Logistic Regression controlling overfitting.\n",
    "            \n",
    "        _num_training - Number of training data points. Used for regularization.\n",
    "        \n",
    "        _lambda - Hyperparameter controlling regularization.\n",
    "        \"\"\"\n",
    "        self.weights = None\n",
    "        self._epsilon = None\n",
    "        self._num_training = None\n",
    "        self._lambda = None\n",
    "        return None\n",
    "    \n",
    "    \n",
    "    def accuracy(self, y, x):\n",
    "        \"\"\"\n",
    "        Returns the classification accuracy.\n",
    "        \"\"\"\n",
    "        prediction = self.predict(x)\n",
    "        return np.mean(prediction == y)\n",
    "    \n",
    "    \n",
    "    def fit(self, y, x, n=1, epsilon=.01, regularization=None, _lambda=1):\n",
    "        \"\"\"\n",
    "        Learn the weights for a Logistic Regression Classifier from the data\n",
    "        \n",
    "        Params\n",
    "        ---------\n",
    "        y : Numpy array\n",
    "            - The binary class labels for the data points\n",
    "        x : Numpy array\n",
    "            - A list of features for each data point\n",
    "        n : int - Default: 1\n",
    "            - The number of iterations for the training algorithm\n",
    "        epsilon : float - Default: 0.01\n",
    "            - Hyperparamter controlling overfitting of the model to the data\n",
    "        regularization : {None, 'L1', 'L2'} - Default: None\n",
    "            - Type of regularization to employ on the model\n",
    "        _lambda : float\n",
    "            - Hyper parameter for regularization\n",
    "        \"\"\"\n",
    "        # Initialize the weight vector\n",
    "        w_0 = np.zeros(x.shape[1])\n",
    "        \n",
    "        # Variables used for learning weights\n",
    "        self._epsilon = epsilon\n",
    "        self._num_training = x.shape[0]\n",
    "        self._lambda = _lambda\n",
    "        \n",
    "        # Pick the correct update method\n",
    "        if regularization == 'l1':\n",
    "            #print 'L1 regularization'\n",
    "            update_func = self._l1\n",
    "        elif regularization == 'l2':\n",
    "            #print 'L2 regularization'\n",
    "            update_func = self._l2\n",
    "        else:\n",
    "            #print 'No regularization'\n",
    "            update_func = self._no_reg\n",
    "                    \n",
    "        # Number of iterations\n",
    "        for _ in range(n):\n",
    "\n",
    "            # Loop over all the data points\n",
    "            for i in range(x.shape[0]):\n",
    "                \n",
    "                y_minus_g = y[i] - sigmoid( np.dot(w_0, x[i]) )\n",
    "                w_0 = update_func(y[i], x[i], y_minus_g, w_0)\n",
    "\n",
    "        # Save the learned weights\n",
    "        self.weights = w_0\n",
    "        return None\n",
    "    \n",
    "    \n",
    "    def _no_reg(self, y, x, y_minus_g, w_0):\n",
    "        \"\"\"\n",
    "        Calculate the update to the weight vector with no regularization.\n",
    "        \n",
    "        Params\n",
    "        ---------\n",
    "        y : {0, 1}\n",
    "            The binary class label for the data point\n",
    "        x : Numpy array\n",
    "            The feature vector for a single data point\n",
    "        y_minus_g : float\n",
    "            (y - sigmoid(w^T x))\n",
    "        w_0 : float\n",
    "            Previous weight vector\n",
    "        epsilon : float\n",
    "            Hyperparameter controlling overfitting\n",
    "            \n",
    "        Returns \n",
    "        ---------\n",
    "            - New weight vector\n",
    "        \"\"\"\n",
    "        # Initialize weight vector to return\n",
    "        w_1 = np.zeros(len(w_0))\n",
    "        \n",
    "        ascent = self._epsilon * y_minus_g\n",
    "        \n",
    "        for j in range(len(x)):\n",
    "            w_1[j] = w_0[j] + x[j]*ascent\n",
    "            \n",
    "        return w_1\n",
    "    \n",
    "    \n",
    "    def _l1(self, y, x, y_minus_g, w_0):\n",
    "        \"\"\"\n",
    "        Update rule for Logistic Regression with L1 regularization.\n",
    "        \n",
    "        Params\n",
    "        ---------\n",
    "        y : {0, 1}\n",
    "            The binary class label for the data point\n",
    "        x : Numpy array\n",
    "            The feature vector for a single data point\n",
    "        y_minus_g : float\n",
    "            (y - sigmoid(w^T x))\n",
    "        w_0 : float\n",
    "            Previous weight vector\n",
    "        epsilon : float\n",
    "            Hyperparameter controlling overfitting\n",
    "            \n",
    "        Returns \n",
    "        ---------\n",
    "            - New weight vector\n",
    "        \"\"\"\n",
    "        # Initialize weight vector to return\n",
    "        w_1 = np.zeros(len(w_0))\n",
    "        \n",
    "        for j in range(len(x)):\n",
    "            reg = float(self._sign(w_0[j])) / (self._lambda*self._num_training)\n",
    "            w_1[j] = w_0[j] + self._epsilon*(x[j]*y_minus_g - reg)\n",
    "            \n",
    "        return w_1\n",
    "    \n",
    "    \n",
    "    def _l2(self, y, x, y_minus_g, w_0):\n",
    "        \"\"\"\n",
    "        Update rule for Logistic Regression with L2 regularization.\n",
    "        \n",
    "        Params\n",
    "        ---------\n",
    "        y : {0, 1}\n",
    "            The binary class label for the data point\n",
    "        x : Numpy array\n",
    "            The feature vector for a single data point\n",
    "        y_minus_g : float\n",
    "            (y - sigmoid(w^T x))\n",
    "        w_0 : float\n",
    "            Previous weight vector\n",
    "        epsilon : float\n",
    "            Hyperparameter controlling overfitting\n",
    "            \n",
    "        Returns \n",
    "        ---------\n",
    "            - New weight vector\n",
    "        \"\"\"\n",
    "        # Initialize weight vector to return\n",
    "        w_1 = np.zeros(len(w_0))\n",
    "        \n",
    "        for j in range(len(x)):\n",
    "            reg = float(w_0[j]) / (self._lambda*self._num_training)\n",
    "            w_1[j] = w_0[j] + self._epsilon*(x[j]*y_minus_g - reg)\n",
    "            \n",
    "        return w_1\n",
    "    \n",
    "    \n",
    "    def _sign(self, number): \n",
    "        \"\"\"\n",
    "        Returns the sign of a number (0 if number == 0)\n",
    "        \"\"\"\n",
    "        return cmp(number,0)\n",
    "    \n",
    "    \n",
    "    def predict(self, data):\n",
    "        \"\"\"\n",
    "        Classifies each data point in x according to the weights learned from the fit method.\n",
    "\n",
    "        Params\n",
    "        ---------\n",
    "        x : Numpy array\n",
    "            The array of data points to classify.\n",
    "            \n",
    "        Returns\n",
    "        ---------\n",
    "            - Predicted class label for each data point.\n",
    "        \"\"\"\n",
    "        prediction = []\n",
    "\n",
    "        for x in data:\n",
    "            prob_0 = self._sigmoidLikelihood(x, 0)\n",
    "            prob_1 = self._sigmoidLikelihood(x, 1)\n",
    "\n",
    "            if prob_0 > prob_1:\n",
    "                prediction.append(0)\n",
    "            else:\n",
    "                prediction.append(1)\n",
    "\n",
    "        return prediction\n",
    "    \n",
    "    \n",
    "    def _sigmoidLikelihood(self, x, label):\n",
    "        \"\"\"\n",
    "        Returns the sigmoid likelihood p(y=label|features; weights)\n",
    "        \n",
    "        x : Numpy array\n",
    "            Feature set for a single data point\n",
    "        \"\"\"\n",
    "        logit = sigmoid(np.dot(x, self.weights))\n",
    "        \n",
    "        if label == 0:\n",
    "            return (1-logit)\n",
    "        elif label == 1:\n",
    "            return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.feature_extraction import DictVectorizer as DV\n",
    "#from sklearn.linear_model import LogisticRegression as sk_lr\n",
    "\n",
    "\n",
    "from get_data import data\n",
    "from logisticregression import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the data set\n",
    "df = data()\n",
    "\n",
    "X = df[ [col for col in df if col not in ['label', 'class']]]\n",
    "y = df['class'].values\n",
    "\n",
    "# Binarize the categorical data using a DictVectorizer\n",
    "# This requires the data be fed in the form of Python dicts\n",
    "vectorizer = DV(sparse=False)\n",
    "X_binarized = vectorizer.fit_transform(X.to_dict(orient='records'))\n",
    "\n",
    "X_binarized = np.array(X_binarized)\n",
    "\n",
    "# Split into train, cv and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_binarized, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon: 0.01\n",
      "L1 regularization\n",
      "Lambda: 1.0\n",
      "Luigi Error: 0.782090652254\n"
     ]
    }
   ],
   "source": [
    "# My implementation\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(y_train, X_train, regularization='l1')\n",
    "accuracy = 1 - classifier.accuracy(y_test, X_test)\n",
    "print('Luigi Error: {}'.format((1-accuracy)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sklearn Error: 0.202554968677\n"
     ]
    }
   ],
   "source": [
    "# sklearn\n",
    "classifier = sk_lr()\n",
    "classifier.fit(X_train, y_train)\n",
    "accuracy = classifier.score(X_test, y_test)\n",
    "print('Sklearn Error: {}'.format((1-accuracy)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-0003f44eb599>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Fit the model to the training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mt0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mt1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-f36ffda0f1df>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, y, x, n, epsilon, regularization, _lambda)\u001b[0m\n\u001b[1;32m     51\u001b[0m         \"\"\"\n\u001b[1;32m     52\u001b[0m         \u001b[0;31m# Initialize the weight vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mw_0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;31m# Variables used for learning weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "# Instantiate the model estimator\n",
    "classifier = LogisticRegression()\n",
    "\n",
    "# Fit the model to the training data\n",
    "t0 = time()\n",
    "classifier.fit(y_train, X_train)\n",
    "t1 = time()\n",
    "\n",
    "print 'Time to train classifier: {} seconds'.format(t1-t0)\n",
    "\n",
    "# Apply the learned model on unseen data\n",
    "prediction = classifier.predict(X_test)\n",
    "\n",
    "accuracy = classifier.score(X_test, y_test)\n",
    "error = (1 - accuracy)\n",
    "\n",
    "print\n",
    "print 'Test accuracy: {}'.format(accuracy)\n",
    "print 'Test error: {}'.format(error)\n",
    "\n",
    "print\n",
    "print 'Train accuracy: {}'.format(classifier.score(X_train, y_train))\n",
    "\n",
    "# View the learned coefficients and the intercept\n",
    "#print(classifier.coef_)\n",
    "#print(classifier.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = scipy.io.loadmat('../hw2/hw2data.mat')\n",
    "trainData = data['trainData']\n",
    "\n",
    "y = trainData[:, 14]\n",
    "x = trainData[:, :14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No regularization\n",
      "Accuracy: 0.74\n"
     ]
    }
   ],
   "source": [
    "classifier = LogisticRegression()\n",
    "classifier.fit(y, x, n=500)\n",
    "acc = classifier.accuracy(y, x)\n",
    "print 'Accuracy: {}'.format(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lambdas = np.linspace(1, 10, num=91)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 regularization\n",
      "Lambda: 1.0 \tAccuracy: 0.66\n",
      "L1 regularization\n",
      "Lambda: 1.1 \tAccuracy: 0.49\n",
      "L1 regularization\n",
      "Lambda: 1.2 \tAccuracy: 0.63\n",
      "L1 regularization\n",
      "Lambda: 1.3 \tAccuracy: 0.67\n",
      "L1 regularization\n",
      "Lambda: 1.4 \tAccuracy: 0.6\n",
      "L1 regularization\n",
      "Lambda: 1.5 \tAccuracy: 0.59\n",
      "L1 regularization\n",
      "Lambda: 1.6 \tAccuracy: 0.69\n",
      "L1 regularization\n",
      "Lambda: 1.7 \tAccuracy: 0.7\n",
      "L1 regularization\n",
      "Lambda: 1.8 \tAccuracy: 0.67\n",
      "L1 regularization\n",
      "Lambda: 1.9 \tAccuracy: 0.67\n",
      "L1 regularization\n",
      "Lambda: 2.0 \tAccuracy: 0.74\n",
      "L1 regularization\n",
      "Lambda: 2.1 \tAccuracy: 0.76\n",
      "L1 regularization\n",
      "Lambda: 2.2 \tAccuracy: 0.48\n",
      "L1 regularization\n",
      "Lambda: 2.3 \tAccuracy: 0.61\n",
      "L1 regularization\n",
      "Lambda: 2.4 \tAccuracy: 0.8\n",
      "L1 regularization\n",
      "Lambda: 2.5 \tAccuracy: 0.54\n",
      "L1 regularization\n",
      "Lambda: 2.6 \tAccuracy: 0.76\n",
      "L1 regularization\n",
      "Lambda: 2.7 \tAccuracy: 0.69\n",
      "L1 regularization\n",
      "Lambda: 2.8 \tAccuracy: 0.69\n",
      "L1 regularization\n",
      "Lambda: 2.9 \tAccuracy: 0.76\n",
      "L1 regularization\n",
      "Lambda: 3.0 \tAccuracy: 0.67\n",
      "L1 regularization\n",
      "Lambda: 3.1 \tAccuracy: 0.57\n",
      "L1 regularization\n",
      "Lambda: 3.2 \tAccuracy: 0.68\n",
      "L1 regularization\n",
      "Lambda: 3.3 \tAccuracy: 0.69\n",
      "L1 regularization\n",
      "Lambda: 3.4 \tAccuracy: 0.69\n",
      "L1 regularization\n",
      "Lambda: 3.5 \tAccuracy: 0.81\n",
      "L1 regularization\n",
      "Lambda: 3.6 \tAccuracy: 0.68\n",
      "L1 regularization\n",
      "Lambda: 3.7 \tAccuracy: 0.7\n",
      "L1 regularization\n",
      "Lambda: 3.8 \tAccuracy: 0.69\n",
      "L1 regularization\n",
      "Lambda: 3.9 \tAccuracy: 0.69\n",
      "L1 regularization\n",
      "Lambda: 4.0 \tAccuracy: 0.68\n",
      "L1 regularization\n",
      "Lambda: 4.1 \tAccuracy: 0.66\n",
      "L1 regularization\n",
      "Lambda: 4.2 \tAccuracy: 0.83\n",
      "L1 regularization\n",
      "Lambda: 4.3 \tAccuracy: 0.81\n",
      "L1 regularization\n",
      "Lambda: 4.4 \tAccuracy: 0.64\n",
      "L1 regularization\n",
      "Lambda: 4.5 \tAccuracy: 0.75\n",
      "L1 regularization\n",
      "Lambda: 4.6 \tAccuracy: 0.8\n",
      "L1 regularization\n",
      "Lambda: 4.7 \tAccuracy: 0.68\n",
      "L1 regularization\n",
      "Lambda: 4.8 \tAccuracy: 0.76\n",
      "L1 regularization\n",
      "Lambda: 4.9 \tAccuracy: 0.68\n",
      "L1 regularization\n",
      "Lambda: 5.0 \tAccuracy: 0.69\n",
      "L1 regularization\n",
      "Lambda: 5.1 \tAccuracy: 0.79\n",
      "L1 regularization\n",
      "Lambda: 5.2 \tAccuracy: 0.68\n",
      "L1 regularization\n",
      "Lambda: 5.3 \tAccuracy: 0.46\n",
      "L1 regularization\n",
      "Lambda: 5.4 \tAccuracy: 0.84\n",
      "L1 regularization\n",
      "Lambda: 5.5 \tAccuracy: 0.48\n",
      "L1 regularization\n",
      "Lambda: 5.6 \tAccuracy: 0.53\n",
      "L1 regularization\n",
      "Lambda: 5.7 \tAccuracy: 0.74\n",
      "L1 regularization\n",
      "Lambda: 5.8 \tAccuracy: 0.69\n",
      "L1 regularization\n",
      "Lambda: 5.9 \tAccuracy: 0.49\n",
      "L1 regularization\n",
      "Lambda: 6.0 \tAccuracy: 0.51\n",
      "L1 regularization\n",
      "Lambda: 6.1 \tAccuracy: 0.5\n",
      "L1 regularization\n",
      "Lambda: 6.2 \tAccuracy: 0.48\n",
      "L1 regularization\n",
      "Lambda: 6.3 \tAccuracy: 0.69\n",
      "L1 regularization\n",
      "Lambda: 6.4 \tAccuracy: 0.6\n",
      "L1 regularization\n",
      "Lambda: 6.5 \tAccuracy: 0.7\n",
      "L1 regularization\n",
      "Lambda: 6.6 \tAccuracy: 0.54\n",
      "L1 regularization\n",
      "Lambda: 6.7 \tAccuracy: 0.7\n",
      "L1 regularization\n",
      "Lambda: 6.8 \tAccuracy: 0.75\n",
      "L1 regularization\n",
      "Lambda: 6.9 \tAccuracy: 0.61\n",
      "L1 regularization\n",
      "Lambda: 7.0 \tAccuracy: 0.68\n",
      "L1 regularization\n",
      "Lambda: 7.1 \tAccuracy: 0.69\n",
      "L1 regularization\n",
      "Lambda: 7.2 \tAccuracy: 0.67\n",
      "L1 regularization\n",
      "Lambda: 7.3 \tAccuracy: 0.56\n",
      "L1 regularization\n",
      "Lambda: 7.4 \tAccuracy: 0.69\n",
      "L1 regularization\n",
      "Lambda: 7.5 \tAccuracy: 0.66\n",
      "L1 regularization\n",
      "Lambda: 7.6 \tAccuracy: 0.7\n",
      "L1 regularization\n",
      "Lambda: 7.7 \tAccuracy: 0.68\n",
      "L1 regularization\n",
      "Lambda: 7.8 \tAccuracy: 0.83\n",
      "L1 regularization\n",
      "Lambda: 7.9 \tAccuracy: 0.8\n",
      "L1 regularization\n",
      "Lambda: 8.0 \tAccuracy: 0.6\n",
      "L1 regularization\n",
      "Lambda: 8.1 \tAccuracy: 0.69\n",
      "L1 regularization\n",
      "Lambda: 8.2 \tAccuracy: 0.74\n",
      "L1 regularization\n",
      "Lambda: 8.3 \tAccuracy: 0.64\n",
      "L1 regularization\n",
      "Lambda: 8.4 \tAccuracy: 0.69\n",
      "L1 regularization\n",
      "Lambda: 8.5 \tAccuracy: 0.75\n",
      "L1 regularization\n",
      "Lambda: 8.6 \tAccuracy: 0.67\n",
      "L1 regularization\n",
      "Lambda: 8.7 \tAccuracy: 0.69\n",
      "L1 regularization\n",
      "Lambda: 8.8 \tAccuracy: 0.6\n",
      "L1 regularization\n",
      "Lambda: 8.9 \tAccuracy: 0.52\n",
      "L1 regularization\n",
      "Lambda: 9.0 \tAccuracy: 0.7\n",
      "L1 regularization\n",
      "Lambda: 9.1 \tAccuracy: 0.54\n",
      "L1 regularization\n",
      "Lambda: 9.2 \tAccuracy: 0.68\n",
      "L1 regularization\n",
      "Lambda: 9.3 \tAccuracy: 0.81\n",
      "L1 regularization\n",
      "Lambda: 9.4 \tAccuracy: 0.78\n",
      "L1 regularization\n",
      "Lambda: 9.5 \tAccuracy: 0.76\n",
      "L1 regularization\n",
      "Lambda: 9.6 \tAccuracy: 0.8\n",
      "L1 regularization\n",
      "Lambda: 9.7 \tAccuracy: 0.68\n",
      "L1 regularization\n",
      "Lambda: 9.8 \tAccuracy: 0.69\n",
      "L1 regularization\n",
      "Lambda: 9.9 \tAccuracy: 0.76\n",
      "L1 regularization\n",
      "Lambda: 10.0 \tAccuracy: 0.71\n"
     ]
    }
   ],
   "source": [
    "for l in lambdas:\n",
    "    classifier = LogisticRegression()\n",
    "    classifier.fit(y, x, n=500, regularization='l1', _lambda=l)\n",
    "    acc = classifier.accuracy(y, x)\n",
    "    print 'Lambda: {} \\tAccuracy: {}'.format(l, acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2 regularization\n",
      "Lambda: 1.0 \tAccuracy: 0.75\n",
      "L2 regularization\n",
      "Lambda: 1.1 \tAccuracy: 0.51\n",
      "L2 regularization\n",
      "Lambda: 1.2 \tAccuracy: 0.61\n",
      "L2 regularization\n",
      "Lambda: 1.3 \tAccuracy: 0.54\n",
      "L2 regularization\n",
      "Lambda: 1.4 \tAccuracy: 0.26\n",
      "L2 regularization\n",
      "Lambda: 1.5 \tAccuracy: 0.55\n",
      "L2 regularization\n",
      "Lambda: 1.6 \tAccuracy: 0.27\n",
      "L2 regularization\n",
      "Lambda: 1.7 \tAccuracy: 0.61\n",
      "L2 regularization\n",
      "Lambda: 1.8 \tAccuracy: 0.55\n",
      "L2 regularization\n",
      "Lambda: 1.9 \tAccuracy: 0.36\n",
      "L2 regularization\n",
      "Lambda: 2.0 \tAccuracy: 0.78\n",
      "L2 regularization\n",
      "Lambda: 2.1 \tAccuracy: 0.63\n",
      "L2 regularization\n",
      "Lambda: 2.2 \tAccuracy: 0.59\n",
      "L2 regularization\n",
      "Lambda: 2.3 \tAccuracy: 0.57\n",
      "L2 regularization\n",
      "Lambda: 2.4 \tAccuracy: 0.27\n",
      "L2 regularization\n",
      "Lambda: 2.5 \tAccuracy: 0.58\n",
      "L2 regularization\n",
      "Lambda: 2.6 \tAccuracy: 0.6\n",
      "L2 regularization\n",
      "Lambda: 2.7 \tAccuracy: 0.6\n",
      "L2 regularization\n",
      "Lambda: 2.8 \tAccuracy: 0.59\n",
      "L2 regularization\n",
      "Lambda: 2.9 \tAccuracy: 0.6\n",
      "L2 regularization\n",
      "Lambda: 3.0 \tAccuracy: 0.6\n",
      "L2 regularization\n",
      "Lambda: 3.1 \tAccuracy: 0.81\n",
      "L2 regularization\n",
      "Lambda: 3.2 \tAccuracy: 0.48\n",
      "L2 regularization\n",
      "Lambda: 3.3 \tAccuracy: 0.61\n",
      "L2 regularization\n",
      "Lambda: 3.4 \tAccuracy: 0.8\n",
      "L2 regularization\n",
      "Lambda: 3.5 \tAccuracy: 0.62\n",
      "L2 regularization\n",
      "Lambda: 3.6 \tAccuracy: 0.62\n",
      "L2 regularization\n",
      "Lambda: 3.7 \tAccuracy: 0.64\n",
      "L2 regularization\n",
      "Lambda: 3.8 \tAccuracy: 0.32\n",
      "L2 regularization\n",
      "Lambda: 3.9 \tAccuracy: 0.27\n",
      "L2 regularization\n",
      "Lambda: 4.0 \tAccuracy: 0.56\n",
      "L2 regularization\n",
      "Lambda: 4.1 \tAccuracy: 0.8\n",
      "L2 regularization\n",
      "Lambda: 4.2 \tAccuracy: 0.28\n",
      "L2 regularization\n",
      "Lambda: 4.3 \tAccuracy: 0.81\n",
      "L2 regularization\n",
      "Lambda: 4.4 \tAccuracy: 0.64\n",
      "L2 regularization\n",
      "Lambda: 4.5 \tAccuracy: 0.6\n",
      "L2 regularization\n",
      "Lambda: 4.6 \tAccuracy: 0.62\n",
      "L2 regularization\n",
      "Lambda: 4.7 \tAccuracy: 0.54\n",
      "L2 regularization\n",
      "Lambda: 4.8 \tAccuracy: 0.65\n",
      "L2 regularization\n",
      "Lambda: 4.9 \tAccuracy: 0.54\n",
      "L2 regularization\n",
      "Lambda: 5.0 \tAccuracy: 0.28\n",
      "L2 regularization\n",
      "Lambda: 5.1 \tAccuracy: 0.66\n",
      "L2 regularization\n",
      "Lambda: 5.2 \tAccuracy: 0.64\n",
      "L2 regularization\n",
      "Lambda: 5.3 \tAccuracy: 0.66\n",
      "L2 regularization\n",
      "Lambda: 5.4 \tAccuracy: 0.69\n",
      "L2 regularization\n",
      "Lambda: 5.5 \tAccuracy: 0.61\n",
      "L2 regularization\n",
      "Lambda: 5.6 \tAccuracy: 0.54\n",
      "L2 regularization\n",
      "Lambda: 5.7 \tAccuracy: 0.64\n",
      "L2 regularization\n",
      "Lambda: 5.8 \tAccuracy: 0.63\n",
      "L2 regularization\n",
      "Lambda: 5.9 \tAccuracy: 0.55\n",
      "L2 regularization\n",
      "Lambda: 6.0 \tAccuracy: 0.82\n",
      "L2 regularization\n",
      "Lambda: 6.1 \tAccuracy: 0.57\n",
      "L2 regularization\n",
      "Lambda: 6.2 \tAccuracy: 0.45\n",
      "L2 regularization\n",
      "Lambda: 6.3 \tAccuracy: 0.63\n",
      "L2 regularization\n",
      "Lambda: 6.4 \tAccuracy: 0.82\n",
      "L2 regularization\n",
      "Lambda: 6.5 \tAccuracy: 0.5\n",
      "L2 regularization\n",
      "Lambda: 6.6 \tAccuracy: 0.64\n",
      "L2 regularization\n",
      "Lambda: 6.7 \tAccuracy: 0.54\n",
      "L2 regularization\n",
      "Lambda: 6.8 \tAccuracy: 0.54\n",
      "L2 regularization\n",
      "Lambda: 6.9 \tAccuracy: 0.62\n",
      "L2 regularization\n",
      "Lambda: 7.0 \tAccuracy: 0.32\n",
      "L2 regularization\n",
      "Lambda: 7.1 \tAccuracy: 0.84\n",
      "L2 regularization\n",
      "Lambda: 7.2 \tAccuracy: 0.56\n",
      "L2 regularization\n",
      "Lambda: 7.3 \tAccuracy: 0.61\n",
      "L2 regularization\n",
      "Lambda: 7.4 \tAccuracy: 0.81\n",
      "L2 regularization\n",
      "Lambda: 7.5 \tAccuracy: 0.39\n",
      "L2 regularization\n",
      "Lambda: 7.6 \tAccuracy: 0.55\n",
      "L2 regularization\n",
      "Lambda: 7.7 \tAccuracy: 0.65\n",
      "L2 regularization\n",
      "Lambda: 7.8 \tAccuracy: 0.69\n",
      "L2 regularization\n",
      "Lambda: 7.9 \tAccuracy: 0.84\n",
      "L2 regularization\n",
      "Lambda: 8.0 \tAccuracy: 0.52\n",
      "L2 regularization\n",
      "Lambda: 8.1 \tAccuracy: 0.64\n",
      "L2 regularization\n",
      "Lambda: 8.2 \tAccuracy: 0.83\n",
      "L2 regularization\n",
      "Lambda: 8.3 \tAccuracy: 0.82\n",
      "L2 regularization\n",
      "Lambda: 8.4 \tAccuracy: 0.66\n",
      "L2 regularization\n",
      "Lambda: 8.5 \tAccuracy: 0.61\n",
      "L2 regularization\n",
      "Lambda: 8.6 \tAccuracy: 0.54\n",
      "L2 regularization\n",
      "Lambda: 8.7 \tAccuracy: 0.65\n",
      "L2 regularization\n",
      "Lambda: 8.8 \tAccuracy: 0.66\n",
      "L2 regularization\n",
      "Lambda: 8.9 \tAccuracy: 0.66\n",
      "L2 regularization\n",
      "Lambda: 9.0 \tAccuracy: 0.66\n",
      "L2 regularization\n",
      "Lambda: 9.1 \tAccuracy: 0.67\n",
      "L2 regularization\n",
      "Lambda: 9.2 \tAccuracy: 0.32\n",
      "L2 regularization\n",
      "Lambda: 9.3 \tAccuracy: 0.58\n",
      "L2 regularization\n",
      "Lambda: 9.4 \tAccuracy: 0.69\n",
      "L2 regularization\n",
      "Lambda: 9.5 \tAccuracy: 0.53\n",
      "L2 regularization\n",
      "Lambda: 9.6 \tAccuracy: 0.67\n",
      "L2 regularization\n",
      "Lambda: 9.7 \tAccuracy: 0.43\n",
      "L2 regularization\n",
      "Lambda: 9.8 \tAccuracy: 0.57\n",
      "L2 regularization\n",
      "Lambda: 9.9 \tAccuracy: 0.67\n",
      "L2 regularization\n",
      "Lambda: 10.0 \tAccuracy: 0.58\n"
     ]
    }
   ],
   "source": [
    "for l in lambdas:\n",
    "    classifier = LogisticRegression()\n",
    "    classifier.fit(y, x, n=500, regularization='l2', _lambda=l)\n",
    "    acc = classifier.accuracy(y, x)\n",
    "    print 'Lambda: {} \\tAccuracy: {}'.format(l, acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
